{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementação de uma Rede Neural com Tensorflow\n",
    "\n",
    "No exemplo a seguir, veremos em ação a uma Rede Neural Convolucional (CNN) em um problema de classificação de imagens. Queremos mostrar o processo de construção de uma rede CNN: quais são as etapas para executar e que raciocínio precisa ser feito para executar um dimensionamento adequado de toda a rede e, claro, como implementá-lo com o TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carreguando e preparamdp os dados do MNIST:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ramonfsilva/anaconda3/envs/py36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defina todos os parâmetros da CNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "training_iters = 100000\n",
    "batch_size = 128\n",
    "display_step = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrada de dados MNIST (cada forma é de 28x28 pixels de matriz):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_input = 784"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O total de 10 classes do MNIST (0-9 dígitos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para reduzir o encaixe, aplicamos a técnica de dropout. Este termo refere-se ao abandono de unidades (oculto, entrada e saída) em uma rede neural. Decidir quais neurônios eliminar é aleatório; Uma maneira é aplicar uma probabilidade, como veremos em nosso código. Por esse motivo, definimos o seguinte parâmetro (a ser ajustado):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout = 0.75 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defina os espaços reservados para o gráfico de entrada. O espaço reservado x contém a entrada de dados MNIST (exatamente 728 pixels):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, n_input])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em seguida, mudamos a forma das imagens de entrada 4D para um tensor, usando o operador de reformulação TensorFlow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "_X = tf.reshape(x, shape=[-1, 28, 28, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A segunda e terceira dimensões correspondem à largura e altura da imagem, enquanto a última dimensão é o número total de canais de cor (no nosso caso 1).\n",
    "\n",
    "Assim, podemos exibir nossa imagem de entrada como um tensor bidimensional, de tamanho 28x28:\n",
    "    \n",
    "![title](img/digit.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tf.placeholder(tf.float32, [None, n_classes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Primeira camada convolucional\n",
    "Cada neurônio da camada oculta é conectado a um pequeno subconjunto do tensor de entrada de dimensão 5x5. Isso implica que a camada oculta terá um tamanho 24x24. Também definimos e inicializamos os tensores de pesos compartilhados e viés compartilhado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc1 = tf.Variable(tf.random_normal([5, 5, 1, 32])) \n",
    "bc1 = tf.Variable(tf.random_normal([32]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lembre-se de que, para reconhecer uma imagem, precisamos de mais do que um mapa de recursos. O número é apenas o número de mapas de recursos que estamos considerando para essa primeira camada. No nosso caso, a camada convolucional é composta por 32 mapas de características.\n",
    "\n",
    "O próximo passo é a construção da primeira camada de convolução, conv1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(img, w, b):\n",
    "  return tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(img, w,strides=[1, 1, 1, 1],padding='SAME'),b))\n",
    "\n",
    "conv1 = conv2d(_X,wc1,bc1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma maneira de representar a camada convolucional, ou seja, conv1, é a seguinte:\n",
    "\n",
    "\n",
    "![title](img/first_hidden_layer.jpg)\n",
    "\n",
    "Após a operação de convolução, impomos a etapa de agrupamento que simplifica as informações de saída da camada convolucional criada anteriormente.\n",
    "\n",
    "Em nosso exemplo, vamos considerar uma região 2x2 da camada de convolução e resumiremos as informações em cada ponto da camada de pooling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_pool(img, k):\n",
    "    return tf.nn.max_pool(img, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='SAME')\n",
    "\n",
    "conv1 = max_pool(conv1, k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A figura a seguir mostra as camadas CNNs após a operação de agrupamento e convolução:\n",
    "\n",
    "![title](img/max_pooling.jpg)\n",
    "\n",
    "A última operação é reduzir o overfitting aplicando os operadores tf.nn.dropout TensorFlow na camada convolucional. Para fazer isso, criamos um marcador de posição para a probabilidade (keep_prob) de que a saída de um neurônio é mantida durante o abandono"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_prob = tf. placeholder(tf.float32)\n",
    "conv1 = tf.nn.dropout(conv1,keep_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segunda camada convolucional\n",
    "Para a segunda camada oculta, devemos aplicar as mesmas operações que a primeira camada, e assim definimos e inicializamos os tensores de pesos compartilhados e viés compartilhado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc2 = tf.Variable(tf.random_normal([5, 5, 32, 64]))\n",
    "bc2 = tf.Variable(tf.random_normal([64]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como você pode notar, esta segunda camada oculta terá 64 recursos para uma janela 5x5, enquanto o número de camadas de entrada será dado a partir da primeira camada obtida pela convolução. Em seguida, aplicamos uma segunda camada ao tensor convolucional conv1, mas desta vez aplicamos 64 conjuntos de filtros 5x5 cada para as 32 camadas conv1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv2 = conv2d(conv1,wc2,bc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isso nos dá 64 arrays 14x14 que reduzimos com o pool máximo para 64 arrays 7x7:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv2 = max_pool(conv2, k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, novamente usamos a operação de dropout:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv2 = tf.nn.dropout(conv2, keep_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/second_hidden_layer.jpg)\n",
    "\n",
    "### Camada densamente conectada\n",
    "Nesta etapa, construímos uma camada densamente conectada que usamos para processar a imagem inteira. Os tensores de peso e polarização são os seguintes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd1 = tf.Variable(tf.random_normal([7*7*64, 1024]))\n",
    "bd1 = tf.Variable(tf.random_normal([1024]))\n",
    "\n",
    "dense1 = tf.reshape(conv2, [-1, wd1.get_shape().as_list()[0]]) \n",
    "dense1 = tf.nn.relu(tf.add(tf.matmul(dense1, wd1),bd1)) \n",
    "dense1 = tf.nn.dropout(dense1, keep_prob) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Camada de Saída\n",
    "\n",
    "A última camada define os tensores wout e bout:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "wout = tf.Variable(tf.random_normal([1024, n_classes]))\n",
    "bout = tf.Variable(tf.random_normal([n_classes]))\n",
    "\n",
    "pred = tf.add(tf.matmul(dense1, wout), bout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testando e treinando o modelo\n",
    "A evidência deve ser convertida em probabilidades para cada uma das 10 classes possíveis (o método é idêntico ao que vimos em _Introdução às Redes Neurais_). Por isso, definimos a função custo, que avalia a qualidade do nosso modelo, aplicando a função softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1280, Minibatch Loss= 28334.269531, Training Accuracy= 0.21094\n",
      "Iter 2560, Minibatch Loss= 12346.429688, Training Accuracy= 0.46875\n",
      "Iter 3840, Minibatch Loss= 15580.212891, Training Accuracy= 0.43750\n",
      "Iter 5120, Minibatch Loss= 8610.929688, Training Accuracy= 0.60938\n",
      "Iter 6400, Minibatch Loss= 6809.586426, Training Accuracy= 0.67969\n",
      "Iter 7680, Minibatch Loss= 8614.549805, Training Accuracy= 0.66406\n",
      "Iter 8960, Minibatch Loss= 5066.708008, Training Accuracy= 0.68750\n",
      "Iter 10240, Minibatch Loss= 5150.553223, Training Accuracy= 0.75000\n",
      "Iter 11520, Minibatch Loss= 4862.318359, Training Accuracy= 0.76562\n",
      "Iter 12800, Minibatch Loss= 4648.060059, Training Accuracy= 0.74219\n",
      "Iter 14080, Minibatch Loss= 3160.381348, Training Accuracy= 0.80469\n",
      "Iter 15360, Minibatch Loss= 1920.942749, Training Accuracy= 0.83594\n",
      "Iter 16640, Minibatch Loss= 3225.904541, Training Accuracy= 0.85938\n",
      "Iter 17920, Minibatch Loss= 2636.375488, Training Accuracy= 0.81250\n",
      "Iter 19200, Minibatch Loss= 2595.320801, Training Accuracy= 0.80469\n",
      "Iter 20480, Minibatch Loss= 1810.617188, Training Accuracy= 0.83594\n",
      "Iter 21760, Minibatch Loss= 3361.302734, Training Accuracy= 0.77344\n",
      "Iter 23040, Minibatch Loss= 1248.619995, Training Accuracy= 0.91406\n",
      "Iter 24320, Minibatch Loss= 2336.255859, Training Accuracy= 0.82812\n",
      "Iter 25600, Minibatch Loss= 1847.403809, Training Accuracy= 0.84375\n",
      "Iter 26880, Minibatch Loss= 1564.099243, Training Accuracy= 0.86719\n",
      "Iter 28160, Minibatch Loss= 1565.929810, Training Accuracy= 0.85156\n",
      "Iter 29440, Minibatch Loss= 1620.990234, Training Accuracy= 0.88281\n",
      "Iter 30720, Minibatch Loss= 1096.431641, Training Accuracy= 0.82812\n",
      "Iter 32000, Minibatch Loss= 1593.411377, Training Accuracy= 0.89844\n",
      "Iter 33280, Minibatch Loss= 1615.937012, Training Accuracy= 0.85938\n",
      "Iter 34560, Minibatch Loss= 873.388672, Training Accuracy= 0.91406\n",
      "Iter 35840, Minibatch Loss= 388.394287, Training Accuracy= 0.93750\n",
      "Iter 37120, Minibatch Loss= 1823.439331, Training Accuracy= 0.84375\n",
      "Iter 38400, Minibatch Loss= 490.709961, Training Accuracy= 0.92188\n",
      "Iter 39680, Minibatch Loss= 722.103943, Training Accuracy= 0.92969\n",
      "Iter 40960, Minibatch Loss= 1524.975586, Training Accuracy= 0.82812\n",
      "Iter 42240, Minibatch Loss= 714.117188, Training Accuracy= 0.91406\n",
      "Iter 43520, Minibatch Loss= 516.134399, Training Accuracy= 0.93750\n",
      "Iter 44800, Minibatch Loss= 827.048462, Training Accuracy= 0.89062\n",
      "Iter 46080, Minibatch Loss= 409.316833, Training Accuracy= 0.96094\n",
      "Iter 47360, Minibatch Loss= 822.433350, Training Accuracy= 0.85156\n",
      "Iter 48640, Minibatch Loss= 1446.067871, Training Accuracy= 0.88281\n",
      "Iter 49920, Minibatch Loss= 948.165039, Training Accuracy= 0.86719\n",
      "Iter 51200, Minibatch Loss= 555.535583, Training Accuracy= 0.94531\n",
      "Iter 52480, Minibatch Loss= 750.604126, Training Accuracy= 0.90625\n",
      "Iter 53760, Minibatch Loss= 221.828629, Training Accuracy= 0.95312\n",
      "Iter 55040, Minibatch Loss= 1231.897095, Training Accuracy= 0.89844\n",
      "Iter 56320, Minibatch Loss= 467.503296, Training Accuracy= 0.92188\n",
      "Iter 57600, Minibatch Loss= 1021.790283, Training Accuracy= 0.92188\n",
      "Iter 58880, Minibatch Loss= 1111.542236, Training Accuracy= 0.89062\n",
      "Iter 60160, Minibatch Loss= 484.209229, Training Accuracy= 0.93750\n",
      "Iter 61440, Minibatch Loss= 1445.712646, Training Accuracy= 0.89844\n",
      "Iter 62720, Minibatch Loss= 458.803223, Training Accuracy= 0.92969\n",
      "Iter 64000, Minibatch Loss= 1391.312256, Training Accuracy= 0.89062\n",
      "Iter 65280, Minibatch Loss= 861.034180, Training Accuracy= 0.92188\n",
      "Iter 66560, Minibatch Loss= 936.080505, Training Accuracy= 0.88281\n",
      "Iter 67840, Minibatch Loss= 853.342468, Training Accuracy= 0.89844\n",
      "Iter 69120, Minibatch Loss= 1527.517334, Training Accuracy= 0.84375\n",
      "Iter 70400, Minibatch Loss= 266.056030, Training Accuracy= 0.95312\n",
      "Iter 71680, Minibatch Loss= 449.545532, Training Accuracy= 0.92969\n",
      "Iter 72960, Minibatch Loss= 771.476929, Training Accuracy= 0.88281\n",
      "Iter 74240, Minibatch Loss= 813.982178, Training Accuracy= 0.89062\n",
      "Iter 75520, Minibatch Loss= 327.055603, Training Accuracy= 0.92188\n",
      "Iter 76800, Minibatch Loss= 711.715149, Training Accuracy= 0.94531\n",
      "Iter 78080, Minibatch Loss= 635.185791, Training Accuracy= 0.89844\n",
      "Iter 79360, Minibatch Loss= 874.389648, Training Accuracy= 0.91406\n",
      "Iter 80640, Minibatch Loss= 100.831451, Training Accuracy= 0.97656\n",
      "Iter 81920, Minibatch Loss= 780.277527, Training Accuracy= 0.89062\n",
      "Iter 83200, Minibatch Loss= 821.021729, Training Accuracy= 0.92969\n",
      "Iter 84480, Minibatch Loss= 719.638306, Training Accuracy= 0.92969\n",
      "Iter 85760, Minibatch Loss= 332.114471, Training Accuracy= 0.92969\n",
      "Iter 87040, Minibatch Loss= 642.805908, Training Accuracy= 0.90625\n",
      "Iter 88320, Minibatch Loss= 352.176025, Training Accuracy= 0.95312\n",
      "Iter 89600, Minibatch Loss= 509.741394, Training Accuracy= 0.94531\n",
      "Iter 90880, Minibatch Loss= 572.720825, Training Accuracy= 0.92188\n",
      "Iter 92160, Minibatch Loss= 677.194092, Training Accuracy= 0.88281\n",
      "Iter 93440, Minibatch Loss= 366.206818, Training Accuracy= 0.95312\n",
      "Iter 94720, Minibatch Loss= 58.291588, Training Accuracy= 0.98438\n",
      "Iter 96000, Minibatch Loss= 609.301025, Training Accuracy= 0.91406\n",
      "Iter 97280, Minibatch Loss= 472.077850, Training Accuracy= 0.92188\n",
      "Iter 98560, Minibatch Loss= 556.923462, Training Accuracy= 0.91406\n",
      "Iter 99840, Minibatch Loss= 1147.973755, Training Accuracy= 0.91406\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.9453125\n"
     ]
    }
   ],
   "source": [
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    step = 1\n",
    "    # Keep training until reach max iterations\n",
    "    while step * batch_size < training_iters:\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        # Fit training using batch data\n",
    "        sess.run(optimizer, feed_dict={x: batch_xs, y: batch_ys, keep_prob: dropout})\n",
    "        if step % display_step == 0:\n",
    "            # Calculate batch accuracy\n",
    "            acc = sess.run(accuracy, feed_dict={x: batch_xs, y: batch_ys, keep_prob: 1.})\n",
    "            # Calculate batch loss\n",
    "            loss = sess.run(cost, feed_dict={x: batch_xs, y: batch_ys, keep_prob: 1.})\n",
    "            print(\"Iter \" + str(step*batch_size) +\\\n",
    "                  \", Minibatch Loss= \" + \\\n",
    "                  \"{:.6f}\".format(loss) + \\\n",
    "                  \", Training Accuracy= \" + \\\n",
    "                  \"{:.5f}\".format(acc))\n",
    "        step += 1\n",
    "    print(\"Optimization Finished!\")\n",
    "    # Calculate accuracy for 256 mnist test images\n",
    "    print(\"Testing Accuracy:\", sess.run(accuracy, feed_dict={x: mnist.test.images[:256], y: mnist.test.labels[:256], keep_prob: 1.}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
